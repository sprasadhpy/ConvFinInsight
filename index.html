<!DOCTYPE html>
<html lang="en">
    
<head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta http-equiv="X-UA-Compatible" content="ie=edge" />
    <title>ConvFinBot: Real-World Financial Insights</title>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;700&display=swap" rel="stylesheet" />
    <link rel="stylesheet" href="style.css" />
</head>

<body>

    <header class="hero">
        <div class="container">
            <img src="logo.jpeg" alt="ConvFinBot Logo" class="logo" /> 
            <h1>Welcome to ConvFinBot</h1>
            <p>Your AI-powered assistant for historical financial insights.</p>
        </div>
    </header>

    <section class="about">
        <div class="container">
            <h2>About the Project</h2>
            <p>
                ConvFinBot answers real-world financial questions based on the
                ConvFinQA dataset using language models.
            </p>
            <ul>
                <li><strong>Data Source:</strong> ConvFinQA</li>
                <li><strong>Technology:</strong> LLM-powered conversational AI</li>
                <li><strong>Purpose:</strong> To provide accurate financial answers to user queries</li>
            </ul>
            <p>
                This project demonstrates an agentic financial question answering system designed for faithful and multi-step reasoning over semi-structured financial data. Key components include:
            </p>
            <ul>
                <li><strong>Data and Domain Understanding:</strong> Incorporates descriptive statistics from the ConvFinQA dataset which is enriched with financial document context to ground agent behavior in real-world reporting structures.</li>
                <li><strong>Agent Architecture:</strong> Built on top of GPT-3.5 Turbo and Unsloth/Qwen1.5-14B the agent engages in multi-turn financial dialogues with capabilities tailored for symbolic and arithmetic reasoning over time-dependent financial variables.</li>
                <li><strong>Hybrid Reasoning Stack:</strong> Integrates Chain-of-Thought (CoT), Faithful CoT (natural language → symbolic chain execution) and a calculator augmented execution engine to bridge language-level reasoning and numerate fidelity.</li>
                <li><strong>Conversational Evaluation:</strong> Benchmarked on over 200 sampled and synthetic financial dialogues using hybrid CoT , extended symbolic reasoning and grounded by intermediate program verification and trace validation using Langchain.</li>
                <li><strong>Prompting Strategy Benchmark:</strong> Explores diverse prompting schemas: zero-shot, CoT, tool-augmented and prompts logged via LangChain instrumentation to trace reasoning fidelity and deviation patterns in financial statements.</li>
                <li><strong>Self-Evaluation Module:</strong> Embeds a reflective agent layer for post hoc verification of answer consistency and reasoning trace alignment, enabling adaptive correction in the loop.</li>
                <li><strong>Extended Inference Control:</strong> Leverages <code>&lt;WAIT&gt;</code> tokens and recent s1 delayed decision primitives to support test-time scaffolding techniques.</li>
                <li><strong>External Knowledge Alignment:</strong> Integrates FMP API to inject real-time financial data, facilitating hallucination avoidance and analysis of agentic failure modes under partial observability.</li>
                <li><strong>Comparative Model Evaluation:</strong> Conducts targeted accuracy assessments between GPT-3.5 and fine-tuned Unsloth Qwen1.5-14B across multi-hop financial reasoning scenarios.</li>
                <li><strong>Optimization Techniques:</strong> Applies Supervised Fine-Tuning (SFT) and Group Relative Policy Optimization (GRPO) via Unsloth to improve trajectory faithfulness under reward-aware alignment signals.</li>
                <li><strong>Retrieval-Augmented:</strong> Implements simple RAG pipelines for supporting contextual grounding enabling the agent to condition on structured and semi-structured financial narratives during answer generation.</li>
            </ul>
        </div>
    </section>

    <section class="dataset-stats">
        <div class="container">
            <h2>Dataset Statistics</h2>
            <p>Below are the statistics for the ConvFinQA dataset used in training and evaluation:</p>
            <ul>
                <li><strong>Train Dialogues:</strong> 3,037 samples</li>
                <li><strong>Train Turns:</strong> 11,104 samples</li>
                <li><strong>Dev Dialogues:</strong> 421 samples</li>
                <li><strong>Dev Turns:</strong> 1,490 samples</li>
                <li><strong>Test Dialogues (Private):</strong> 434 samples</li>
                <li><strong>Test Turns (Private):</strong> 1,521 samples</li>
            </ul>
        </div>
    </section>

    <section class="sample-dataset">
        <div class="container">
            <h2>Sample Dataset Example</h2>
            <p><strong>Document:</strong> JKHY/2009/page_28.pdf</p>
            <p><strong>QA Question:</strong> What was the percentage change in the net cash from operating activities from 2008 to 2009?</p>
            <p><strong>Answer:</strong> 14.1%</p>

            <hr />

            <h3>Pre-Text (Excerpt)</h3>
            <pre>
- 26 | 2009 annual report in fiscal 2008, revenues in the credit union systems and services business segment increased 14% from fiscal 2007.
- All revenue components within the segment experienced growth during fiscal 2008.
- License revenue generated the largest dollar growth in revenue as Episys AE, our flagship core processing system aimed at larger credit unions, experienced strong sales throughout the year.
- Support and service revenue, the largest component of total revenues for the credit union segment, experienced 34% growth in EFT support and 10% growth in in-house support.
- Gross profit in this segment increased $9,344 in fiscal 2008 compared to fiscal 2007, due primarily to increased license revenue, which carries the highest margins.
            </pre>

            <h3>Post-Text (Excerpt)</h3>
            <pre>
- Year ended June 30: cash provided by operations increased $25,587 to $206,588 for the fiscal year ended June 30, 2009, compared to $181,001 in 2008.
- This increase is primarily due to a decrease in receivables of $21,214.
- Early software maintenance billing in fiscal 2010 led to higher cash collection before year-end.
- More cash was collected for revenue to be recognized in subsequent periods than in fiscal 2008.
- Cash used in investing activities for fiscal year 2009 was $59,227, including $3,027 in contingent consideration paid on prior acquisitions.
            </pre>

            <h3>Financial Table (Excerpt from PDF)</h3>
            <pre>
Net Cash from Operating Activities:
2009: $206,588
2008: $181,001
            </pre>

            <h3>Reasoning Steps</h3>
            <ul>
                <li>Difference = 206,588 − 181,001 = 25,587</li>
                <li>Percentage Change = (25,587 / 181,001) × 100 = 14.1%</li>
            </ul>

            <h3>Dialogue Breakdown</h3>
            <ol>
                <li>Q0: What is the net cash from operating activities in 2009?</li>
                <li>Q1: What about in 2008?</li>
                <li>Q2: What is the difference?</li>
                <li>Q3: What percentage change does this represent?</li>
            </ol>
        </div>
    </section>

    <section class="dataset-table">
  <div class="container">
    <h2>ConvFinQA Dataset Summary</h2>
    <table border="1" cellspacing="0" cellpadding="8" style="width: 100%; border-collapse: collapse; font-family: sans-serif;">
      <thead style="background-color: #f0f0f0;">
        <tr>
          <th>Dataset</th>
          <th># Conversations</th>
          <th># Questions</th>
          <th>Report Pages</th>
          <th>Vocabulary Size</th>
          <th>Avg. Questions/Conversation</th>
          <th>Avg. Question Length</th>
          <th>Avg. # Sentences in Text</th>
          <th>Avg. # Rows in Table</th>
          <th>Avg. Tokens (Text + Table)</th>
          <th>Max. Tokens (Text + Table)</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td>Train</td>
          <td>3,037</td>
          <td>11,104</td>
          <td>20</td>
          <td>15,524</td>
          <td>3.66</td>
          <td>10.64</td>
          <td>33.89</td>
          <td>5.37</td>
          <td>558.29</td>
          <td>2,152</td>
        </tr>
        <tr>
          <td>Dev</td>
          <td>421</td>
          <td>1,490</td>
          <td>18</td>
          <td>6,386</td>
          <td>3.54</td>
          <td>10.95</td>
          <td>31.11</td>
          <td>5.32</td>
          <td>526.87</td>
          <td>1,352</td>
        </tr>
        <tr>
          <td>Test Private</td>
          <td>434</td>
          <td>1,521</td>
          <td>20</td>
          <td>6,921</td>
          <td>3.50</td>
          <td>10.70</td>
          <td>33.41</td>
          <td>5.58</td>
          <td>537.62</td>
          <td>1,170</td>
        </tr>
      </tbody>
    </table>
  </div>
</section>
    <section class="dataset-visualizations">
  <div class="container">
    <h2>ConvFinQA Dataset Visualizations</h2>

    <div class="viz-block">
      <h3>Dataset Statistics (Train, Dev, Test)</h3>
      <img src="image1.png" alt="ConvFinQA Dataset Statistics" style="max-width: 100%; border: 1px solid #ccc; padding: 5px;" />
    </div>

    <div class="viz-block">
      <h3>Distribution of Longest Dependency Distances</h3>
      <img src="image2.png" alt="Longest Dependency Distance Distribution" style="max-width: 100%; border: 1px solid #ccc; padding: 5px;" />
    </div>

    <div class="summary-text" style="margin: 20px 0; font-size: 16px;">
      <p><strong>Stock Market Keyword Matches:</strong> 1,982</p>
      <p><strong>Corporate Finance Keyword Matches:</strong> 5,534</p>
      <p><strong>Questions with dependency distance &gt; 1:</strong> 2,465 / 11,104 &nbsp; (~22.20%)</p>
       <p><strong>Download summary:</strong> 
        <a href="convfinqa_summary_export.xlsx" download>Click here to download convfinqa_summary_export</a>
      </p>
    
      <p><strong>Included Columns in CSV:</strong></p>
      <ul>
        <li>Document</li>
        <li>QA Question</li>
        <li>Answer</li>
        <li>Reasoning Steps</li>
      </ul>
    </div>

    <div class="viz-block">
      <h3>Stock vs Corporate Finance Term Usage</h3>
      <img src="image3.png" alt="Stock vs Corporate Finance Terms" style="max-width: 100%; border: 1px solid #ccc; padding: 5px;" />
    </div>
  </div>
</section>
    <section class="dataset-breakdown">
  <div class="container">
    <h2>Descriptive Statistics ConvFinQA</h2>
    
    <img src="image4.png" alt="ConvFinQA Breakdown Visualization" style="max-width: 100%; border: 1px solid #ccc; padding: 5px; margin-top: 10px;" />
    
    <p style="font-size: 14px; color: #666; margin-top: 8px;">
      This visualization summarizes question reasoning types, operator usage, company/year coverage, frequent answer units, and top-mentioned companies in the ConvFinQA dataset.
    </p>
  </div>
</section>

    <section class="model-summary">
  <div class="container">
    <h2>LLM, CoT and Faithful CoT for ConvFinQA</h2>

    <p>
      The ConvFinQA system uses <strong>GPT-3.5 Turbo</strong> via LangChain for answering multi-step financial questions grounded in document context and table data.  Model is prompted in two styles:
      i) a direct answering mode and ii) a symbolic reasoning mode that produces evaluable math expressions.
    </p>

    <h3>LLM Details</h3>
    <ul>
      <li><strong>Model:</strong> GPT-3.5 Turbo</li>
      <li><strong>Interface:</strong> LangChain + LangSmith Tracing</li>
      <li><strong>Temperature:</strong> 0.2 (low for factual precision)</li>
    </ul>

    <h3>Prompting Strategies</h3>
    <p>The agent is guided using system instructions and annotated answer formatting:</p>

    <h4> Prompt (LLM-only):</h4>
    <pre style="background:#f4f4f4; padding:10px; border:1px solid #ccc; overflow-x: auto;">
You are a financial analyst assistant. Use the context below to answer the question.
Directly provide the final answer in &lt;ANSWER&gt;...&lt;/ANSWER&gt;.

Context:
[pre-text]

Table:
[table-text]

[post-text]

Question: [question]
Answer:
    </pre>

    <h4> Prompt (LLM + Calculator):</h4>
    <pre style="background:#f4f4f4; padding:10px; border:1px solid #ccc; overflow-x: auto;">
You are a financial analyst assistant. Use the context below to answer the question.
If calculations are needed, enclose the math expression in &lt;MATH&gt;...&lt;/MATH&gt;
and the result in &lt;ANSWER&gt;...&lt;/ANSWER&gt;.

Context:
[pre-text]

Table:
[table-text]

[post-text]

Question: [question]
Answer:
    </pre>

    <h3>Calculator Integration</h3>
    <p>
      Two versions of a calculator were tested for evaluating the math expressions generated by the LLM inside <code>&lt;MATH&gt;...&lt;/MATH&gt;</code> tags:
    </p>

    <h4> Version 1: Python <code>eval()</code> (Baseline)</h4>
    <p>
      This approach directly evaluated expressions like <code>206588 - 181001</code> using Python's built-in <code>eval()</code>:
    </p>

    <pre style="background:#f4f4f4; padding:10px; border:1px solid #ccc;">
def evaluate_expression(expr):
    try:
        return eval(expr, {"__builtins__": {}})
    except:
        return None
    </pre>

    <p>
      While effective for simple arithmetic this method poses potential safety risks if the input is not controlled.
    </p>

    <h4> Version 2: SymPy (Safe Evaluator)</h4>
    <p>
      The improved version uses the <code>sympy</code> library for safe symbolic math evaluation. It parses the expression and evaluates it numerically:
    </p>

    <pre style="background:#f4f4f4; padding:10px; border:1px solid #ccc;">
from sympy import sympify

def evaluate_expression(expr):
    try:
        result = sympify(expr).evalf()
        return float(result)
    except:
        return None
    </pre>

    <p>
      This is safer more transparent and supports symbolic computation when needed.Making it better suited for multi-step financial reasoning.
    </p>

    <h3>Why This Setup?</h3>
    <ul>
      <li>Ensures <strong>faithful reasoning</strong> over both text and tabular inputs</li>
      <li>Separates <strong>natural language</strong> and <strong>numerical computation</strong></li>
      <li>Enables symbolic traceability and numerical post-verification</li>
      <li>Improves precision on financial ratios and arithmetic operations</li>
    </ul>
  </div>
</section>


    <div class="container">
    <h1>Experiment 1: Prompting Evaluation on ConvFinQA</h1>

    <p>
      We perform a structured evaluation of <strong>GPT-3.5 Turbo</strong> on the <strong>ConvFinQA</strong> dataset by comparing two prompting strategies: 
      (1) a direct answer mode using <code>&lt;ANSWER&gt;...&lt;/ANSWER&gt;</code> and 
      (2) a symbolic reasoning mode that embeds expressions in <code>&lt;MATH&gt;...&lt;/MATH&gt;</code> tags followed by their result.
    </p>

    <p>
      It uses <code>LangChain</code>’s <code>ChatOpenAI</code> interface with <code>LangSmith</code> tracing to log each interaction. 
      The script samples 200 QA pairs from <code>train_turn.json</code>, generates prompts including pre-text, post-text, and flattened financial tables, 
      and invokes the LLM with both prompt types. If a <code>&lt;MATH&gt;</code> expression is produced it is evaluated using Python’s <code>eval()</code> in a restricted context. 
      Predicted answers are extracted, matched against gold labels using relative (3%) and absolute (0.05) tolerances, 
      and logged alongside latency, conversation type, and trace metadata.<p> Final traces with results are exported to a txt file (<code>Experiment1</code>) for accuracy analysis across conversation styles and reasoning modes.
    </p>

    <p>
  <strong>Download full trace output:</strong><br />
  <a href="Experiment1.rtf" download>Click here to download Experiment1.txt</a>
   </p>

    </p>
  </div>
    <section class="experiment-figure">
  <div class="container">
    <h2>Figure 1: Accuracy Comparison</h2>
    <img src="Experiment1.png" alt="Accuracy Comparison Bar Chart" style="max-width:100%; height:auto; border:1px solid #ccc; border-radius:8px;" />
    <p style="font-size: 14px; color: #555;">Figure: Comparison of LLM-only vs LLM + Calculator accuracy of GPT3.5 turbo on 200 ConvFinQA questions.</p>
  </div>
</section>

      <section class="prompt-strategies">
  <div class="container">
    <h2>Experiment 2: Prompting Strategies explored</h2>
    <p>Below are the five distinct prompting strategies evaluated on the ConvFinQA dataset:</p>

    <h3>1. ExplicitCoT (Chain-of-Thought)</h3>
    <p>
      This method guides the model to reason step-by-step before answering and thereby improving multi-hop question accuracy.  
      Introduced in <a href="https://arxiv.org/abs/2201.11903" target="_blank">Wei et al., 2022</a> (Chain of Thought Prompting).
    </p>
    <pre style="background:#f9f9f9; padding:12px; border:1px solid #ccc; border-radius:6px;">
You are a financial analyst assistant. Explain your reasoning step by step before answering.

Context:
[pre_text]

Table:
[table_text]

[post_text]

Question: [question]
Answer:
    </pre>

    <h3>2. ReflectionCoT</h3>
    <p>
      Encourages the model to reflect and reason before committing to an answer and  helping correct early-stage reasoning.  
      Based on <a href="https://arxiv.org/abs/2303.17651" target="_blank">Madaan et al., 2023</a> (Self-Refine).
    </p>
    <pre style="background:#f9f9f9; padding:12px; border:1px solid #ccc; border-radius:6px;">
Think through the question carefully. Reflect on each aspect of the question and explain your steps before answering.

Context:
[pre_text]

Table:
[table_text]

[post_text]

Question: [question]
Answer:
    </pre>

    <h3>3. Scratchpad</h3>
    <p>
      Provides space for the model to "think aloud" or record intermediate computations before giving the final output.  
      Inspired by <a href="https://arxiv.org/abs/2112.00114" target="_blank">Nye et al., 2021</a> (Scratchpads for Intermediate Computation).
    </p>
    <pre style="background:#f9f9f9; padding:12px; border:1px solid #ccc; border-radius:6px;">
Use the scratchpad below to write down intermediate steps, then give the final answer.

Context:
[pre_text]

Table:
[table_text]

[post_text]

Question: [question]
Answer:
    </pre>

    <h3>4. PoT (Program-of-Thought with Math Evaluation)</h3>
    <p>
      Separates language from computation by wrapping expressions in &lt;MATH&gt; and &lt;ANSWER&gt; tags for symbolic evaluation.  
      Proposed in <a href="https://arxiv.org/abs/2211.12588" target="_blank">Chen et al., 2022</a> (Program of Thoughts Prompting).
    </p>
    <pre style="background:#f9f9f9; padding:12px; border:1px solid #ccc; border-radius:6px;">
You are a financial assistant. For all math expressions, wrap them in &lt;MATH&gt;...&lt;/MATH&gt; 
and the final result in &lt;ANSWER&gt;...&lt;/ANSWER&gt;.

Context:
[pre_text]

Table:
[table_text]

[post_text]

Question: [question]
Answer:
    </pre>

    <h3>5. EEDP (Explicit Expression Decomposition Prompting)</h3>
    <p>
      Breaks down complex queries into sub-questions to improve modular reasoning and intermediate verification.  
      Inspired by <a href="https://arxiv.org/abs/2210.02406" target="_blank">Khot et al., 2022</a> (Decomposed Prompting).
    </p>
    <pre style="background:#f9f9f9; padding:12px; border:1px solid #ccc; border-radius:6px;">
Break the question into sub-questions answer each part then combine results to answer the original question.

Context:
[pre_text]

Table:
[table_text]

[post_text]

Question: [question]
Answer:
    </pre>
  </div>
</section>

    <div class="container"> 
    <h2>Experiment3: Self-Reflective Evaluation </h2>
        <p>
       We implement a <strong>self-reflective evaluation loop</strong> using <strong>GPT-3.5 Turbo</strong> to enhance its reasoning fidelity on complex financial QA tasks. 
            The model is queried via LangChain’s <code>ChatOpenAI</code> interface using a structured prompt format that supports symbolic reasoning through <code>&lt;MATH&gt;</code> and <code>&lt;ANSWER&gt;</code> tags. After generating an initial answer it is compared against the ground truth using numerical thresholds (3% relative, 0.05 absolute). 
            If incorrect then the agent is prompted to critique its own reasoning and this reflection is incorporated into a follow up prompt to encourage correction. 
            This creates a lightweight feedback loop that enables self-improvement without human intervention. 
            Traces are logged via <strong>LangSmith</strong> for full reproducibility and final results  including predictions, retries, and correctness metrics  are exported as a CSV file: <a href="langchain_self_eval-2.csv" download>langchain_self_eval.csv</a>. 
            This approach draws inspiration from the work <a href="https://arxiv.org/pdf/2203.11171" target="_blank">"Self-consistency improves for Chain-of-Thought Reasoning" (Wang et al., 2023)</a> and enables scalable evaluation of multi-step financial reasoning with minimal supervision.
            <section class="result-comparison">
              <div class="container" style="text-align: center; padding: 20px;">
                <h2>Figure 3: How does Self reflection helps ? </h2>
                <p style="font-size: 14px; color: #444;">
                  This figure compares the performance of three reasoning strategies on ConvFinQA. self-evaluating agent achieves the highest accuracy.
                </p>
                <img src="Experiment3.png" alt="Accuracy Comparison of Strategies" style="width: 70%; max-width: 600px; border: 1px solid #ccc; border-radius: 8px; box-shadow: 0 2px 6px rgba(0,0,0,0.1);" />
              </div>
            </section>

</p>

    </div>
<div class="container"> 
  <section>
    <h2>Experiment 4: Extended Reasoning with the “Wait...” Scaffolding Technique</h2>
    <p>
      Inspired by the <em>"s1: Simple Test-Time Scaling"</em> paper 
      (<a href="https://arxiv.org/abs/2501.19393" target="_blank">arXiv:2501.19393</a>), 
      which demonstrated how prompting with a simple “Wait...” token can significantly enhance reasoning in large language models, 
      I applied a similar scaffolding method to my financial QA system built on the ConvFinQA dataset. 
      In this setup, the model was instructed to begin responses with “Wait...” and enclose final answers within 
      <code>&lt;ANSWER&gt;</code> tags to encourage structured, step-by-step reasoning. 
      This intervention resulted in a <strong>notable accuracy improvement from 48.00% to 56.50%</strong>, 
      marking an <strong>8.50% gain</strong> through self-reflective retries.
    </p>

    <p>
      However, this performance still lagged behind the <strong>67.5% accuracy</strong> achieved by a dedicated reasoning agent. 
      One likely reason for this gap is that the “Wait...” prompt enforces general deliberation but doesn’t instill 
      domain-specific financial reasoning or leverage external tools, which the agent was explicitly trained to use. 
      Additionally, discrepancies in question sets or difficulty levels between experiments may have influenced the outcome, 
      making direct comparisons imperfect. These results suggest that while lightweight test-time scaffolding improves robustness, 
      its gains are bounded by task complexity and the model's reasoning priors.
    </p>

    <p>
      The evaluation results are saved in the file: 
      <a href="langchain_self_eval_waittoken.csv" download>
        <code>langchain_self_eval_waittoken.csv</code>
      </a>.
    </p>

    <h2>Accuracy Comparison: Self-Reflection vs Scaffolding Technique</h2>
    <p>
      The bar chart below compares the performance of various strategies, including LLM-only, LLM with calculator, 
      self-evaluating agents, and the "Wait..." scaffolding method.
    </p>
    <img src="assets/Experiment4.png" alt="Accuracy Comparison Graph" 
         style="max-width: 100%; height: auto; border: 1px solid #ccc; border-radius: 8px; padding: 4px;">
  </section>
</div>




    <section class="demo">
        <div class="container">
            <h2>Try ConvFinBot</h2>
            <p>
                Ask your financial question and the bot will give you insights based on real financial documents.
            </p>
            <button onclick="openChat()">Start Chat</button>
        </div>
    </section>

    <footer>
        <div class="container">
            <p>&copy; 2025 ConvFinBot. All rights reserved.</p>
            <p>
                View the code on
                <a href="https://github.com/sprasadhpy/ConvFinInsight" target="_blank">GitHub</a>
            </p>
        </div>
    </footer>

    <script>
        function openChat() {
            alert("The chatbot feature is under development.");
        }
    </script>
</body>

</html>
<!-- trigger redeploy -->
<!-- trigger deploy -->

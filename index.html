<!DOCTYPE html>
<html lang="en">
    
<head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta http-equiv="X-UA-Compatible" content="ie=edge" />
    <title>ConvFinBot: Real-World Financial Insights</title>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;700&display=swap" rel="stylesheet" />
    <link rel="stylesheet" href="style.css" />
</head>

<body>

    <header class="hero">
        <div class="container">
            <img src="logo.jpeg" alt="ConvFinBot Logo" class="logo" /> 
            <h1>Welcome to ConvFinBot</h1>
            <p>Your AI-powered assistant for historical financial insights.</p>
            <p>
                View the code on
               <small> <a href="https://github.com/sprasadhpy/ConvFinInsight" target="_blank">GitHub</a></small>
            </p>
        </div>
    </header>

    <section class="about">
        <div class="container">
            <h2>About the Project</h2>
            <p>
                ConvFinBot answers real-world financial questions based on the
                ConvFinQA dataset using language models.
            </p>
           <ul>
                <li><strong>Data Source:</strong> <a href="https://github.com/czyssrs/ConvFinQA" target="_blank">ConvFinQA</a></li>
                <li><strong>Technology:</strong> LLM-powered conversational AI</li>
                <li><strong>Purpose:</strong> To provide accurate financial answers to user queries</li>
            </ul>

            <p>
                This project demonstrates an agentic financial question answering system designed for faithful and multi-step reasoning over semi-structured financial data. Key components include:
            </p>
            <ul>
                <li><strong>Data and Domain Understanding:</strong> Incorporates descriptive statistics from the ConvFinQA dataset which is enriched with financial document context to ground agent behavior in real-world reporting structures.</li>
                <li><strong>Agent Architecture:</strong> Built on top of GPT-3.5 Turbo and Unsloth/Qwen1.5-14B the agent engages in multi-turn financial dialogues with capabilities tailored for symbolic and arithmetic reasoning over time-dependent financial variables.</li>
                <li><strong>Hybrid Reasoning Stack:</strong> Integrates Chain-of-Thought (CoT), Faithful CoT (natural language → symbolic chain execution) and a calculator augmented execution engine to bridge language-level reasoning and numerate fidelity.</li>
                <li><strong>Conversational Evaluation:</strong> Benchmarked on over 200 sampled and synthetic financial dialogues using hybrid CoT and extended symbolic reasoning and grounded by intermediate program verification and trace validation using Langchain.</li>
                <li><strong>Prompting Strategy Benchmark:</strong> Explores diverse prompting schemas: zero-shot, CoT, tool-augmented and prompts logged via LangChain instrumentation to trace reasoning fidelity and deviation patterns in financial statements.</li>
                <li><strong>Self-Evaluation Module:</strong> Embeds a reflective agent layer for post hoc verification of answer consistency and reasoning trace alignment and also enabling adaptive correction in the loop.</li>
                <li><strong>Extended Inference Control:</strong> Leverages <code>&lt;WAIT&gt;</code> tokens and recent s1 delayed decision primitives to support test-time scaffolding techniques.</li>
                <li><strong>External Knowledge Alignment:</strong> Integrates FMP API to inject real-time financial data, facilitating hallucination avoidance and analysis of agentic failure modes under partial observability.</li>
                <li><strong>Comparative Model Evaluation:</strong> Conducts targeted accuracy assessments between GPT-3.5 and fine-tuned Unsloth Qwen1.5-14B across multi-hop financial reasoning scenarios.</li>
                <li><strong>Optimization Techniques:</strong> Applies Supervised Fine-Tuning (SFT) and Group Relative Policy Optimization (GRPO) via Unsloth to improve trajectory faithfulness under reward-aware alignment signals.</li>
                <li><strong>Retrieval-Augmented:</strong> Implements simple RAG pipelines for supporting contextual grounding enabling the agent to condition on structured and semi-structured financial narratives during answer generation.</li>
            </ul>
        </div>
    </section>
     <br>
        <section class="framework">
            <div class="container" style="max-width: 960px; margin: auto; text-align: center;">
                <h2 style="margin-bottom: 16px;">ConvFinQA + Agent Framework</h2>
                <img src="Framewor.png" alt="ConvFinQA + Agent Framework Diagram" style="width: 100%; height: auto; border-radius: 8px; box-shadow: 0 4px 12px rgba(0, 0, 0, 0.1);">
                <p style="font-size: 0.9rem; color: #555; margin-top: 10px;">  <em>Zoom the page (Ctrl + or ⌘ +) to view the image clearly</em>
            </div>
        </section>
        <br>

    

    <section class="dataset-stats">
        <div class="container">
            <h2>Dataset Statistics</h2>
            <p>Below are the statistics for the ConvFinQA dataset used in training and evaluation:</p>
            <ul>
                <li><strong>Train Dialogues:</strong> 3,037 samples</li>
                <li><strong>Train Turns:</strong> 11,104 samples</li>
                <li><strong>Dev Dialogues:</strong> 421 samples</li>
                <li><strong>Dev Turns:</strong> 1,490 samples</li>
                <li><strong>Test Dialogues (Private):</strong> 434 samples</li>
                <li><strong>Test Turns (Private):</strong> 1,521 samples</li>
            </ul>
        </div>
    </section>

    <section class="sample-dataset">
        <div class="container">
            <h2>Sample Dataset Example</h2>
            <p><strong>Document:</strong> JKHY/2009/page_28.pdf</p>
            <p><strong>QA Question:</strong> What was the percentage change in the net cash from operating activities from 2008 to 2009?</p>
            <p><strong>Answer:</strong> 14.1%</p>

            <hr />

            <h3>Pre-Text (Excerpt)</h3>
            <pre>
            - 26 | 2009 annual report in fiscal 2008, revenues in the credit union systems and services business segment increased 14% from fiscal 2007.
            - All revenue components within the segment experienced growth during fiscal 2008.
            - License revenue generated the largest dollar growth in revenue as Episys AE, our flagship core processing system aimed at larger credit unions, experienced strong sales throughout the year.
            - Support and service revenue, the largest component of total revenues for the credit union segment, experienced 34% growth in EFT support and 10% growth in in-house support.
            - Gross profit in this segment increased $9,344 in fiscal 2008 compared to fiscal 2007, due primarily to increased license revenue, which carries the highest margins.
                        </pre>
            
                        <h3>Post-Text (Excerpt)</h3>
                        <pre>
            - Year ended June 30: cash provided by operations increased $25,587 to $206,588 for the fiscal year ended June 30, 2009, compared to $181,001 in 2008.
            - This increase is primarily due to a decrease in receivables of $21,214.
            - Early software maintenance billing in fiscal 2010 led to higher cash collection before year-end.
            - More cash was collected for revenue to be recognized in subsequent periods than in fiscal 2008.
            - Cash used in investing activities for fiscal year 2009 was $59,227, including $3,027 in contingent consideration paid on prior acquisitions.
            </pre>

            <h3>Financial Table (Excerpt from PDF)</h3>
            <pre>
            Net Cash from Operating Activities:
            2009: $206,588
            2008: $181,001
            </pre>

            <h3>Reasoning Steps</h3>
            <ul>
                <li>Difference = 206,588 − 181,001 = 25,587</li>
                <li>Percentage Change = (25,587 / 181,001) × 100 = 14.1%</li>
            </ul>

            <h3>Dialogue Breakdown</h3>
            <ol>
                <li>Q0: What is the net cash from operating activities in 2009?</li>
                <li>Q1: What about in 2008?</li>
                <li>Q2: What is the difference?</li>
                <li>Q3: What percentage change does this represent?</li>
            </ol>
        </div>
    </section>

    <section class="dataset-table">
  <div class="container">
    <h2>ConvFinQA Dataset Summary</h2>
    <table border="1" cellspacing="0" cellpadding="8" style="width: 100%; border-collapse: collapse; font-family: sans-serif;">
      <thead style="background-color: #f0f0f0;">
        <tr>
          <th>Dataset</th>
          <th># Conversations</th>
          <th># Questions</th>
          <th>Report Pages</th>
          <th>Vocabulary Size</th>
          <th>Avg. Questions/Conversation</th>
          <th>Avg. Question Length</th>
          <th>Avg. # Sentences in Text</th>
          <th>Avg. # Rows in Table</th>
          <th>Avg. Tokens (Text + Table)</th>
          <th>Max. Tokens (Text + Table)</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td>Train</td>
          <td>3,037</td>
          <td>11,104</td>
          <td>20</td>
          <td>15,524</td>
          <td>3.66</td>
          <td>10.64</td>
          <td>33.89</td>
          <td>5.37</td>
          <td>558.29</td>
          <td>2,152</td>
        </tr>
        <tr>
          <td>Dev</td>
          <td>421</td>
          <td>1,490</td>
          <td>18</td>
          <td>6,386</td>
          <td>3.54</td>
          <td>10.95</td>
          <td>31.11</td>
          <td>5.32</td>
          <td>526.87</td>
          <td>1,352</td>
        </tr>
        <tr>
          <td>Test Private</td>
          <td>434</td>
          <td>1,521</td>
          <td>20</td>
          <td>6,921</td>
          <td>3.50</td>
          <td>10.70</td>
          <td>33.41</td>
          <td>5.58</td>
          <td>537.62</td>
          <td>1,170</td>
        </tr>
      </tbody>
    </table>
  </div>
</section>
    <section class="dataset-visualizations">
  <div class="container">
    <h2>ConvFinQA Dataset Visualizations</h2>

    <div class="viz-block">
      <h3>Dataset Statistics (Train, Dev, Test)</h3>
      <img src="image1.png" alt="ConvFinQA Dataset Statistics" style="max-width: 100%; border: 1px solid #ccc; padding: 5px;" />
    </div>

    <div class="viz-block">
      <h3>Distribution of Longest Dependency Distances</h3>
      <img src="image2.png" alt="Longest Dependency Distance Distribution" style="max-width: 100%; border: 1px solid #ccc; padding: 5px;" />
    </div>

    <div class="summary-text" style="margin: 20px 0; font-size: 16px;">
      <p><strong>Stock Market Keyword Matches:</strong> 1,982</p>
      <p><strong>Corporate Finance Keyword Matches:</strong> 5,534</p>
      <p><strong>Questions with dependency distance &gt; 1:</strong> 2,465 / 11,104 &nbsp; (~22.20%)</p>
       <p><strong>Download summary:</strong> 
        <a href="convfinqa_summary_export.xlsx" download>Click here to download convfinqa_summary_export</a>
      </p>
    
      <p><strong>Included Columns in CSV:</strong></p>
      <ul>
        <li>Document</li>
        <li>QA Question</li>
        <li>Answer</li>
        <li>Reasoning Steps</li>
      </ul>
    </div>

    <div class="viz-block">
      <h3>Stock vs Corporate Finance Term Usage</h3>
      <img src="image3.png" alt="Stock vs Corporate Finance Terms" style="max-width: 100%; border: 1px solid #ccc; padding: 5px;" />
    </div>
  </div>
</section>
    <section class="dataset-breakdown">
  <div class="container">
    <h2>Descriptive Statistics ConvFinQA</h2>
    
    <img src="image4.png" alt="ConvFinQA Breakdown Visualization" style="max-width: 100%; border: 1px solid #ccc; padding: 5px; margin-top: 10px;" />
    
    <p style="font-size: 14px; color: #666; margin-top: 8px;">
      This visualization summarizes question reasoning types, operator usage, company/year coverage, frequent answer units, and top-mentioned companies in the ConvFinQA dataset.
    </p>
      <p> This comprehensive dataset diagnostic provides a multi-faceted view of the <strong>ConvFinQA corpus</strong> shedding light on structural and semantic patterns that influence multi-agent reasoning performance. As seen in <strong>Panel 1</strong> the question type distribution is heavily skewed towards percentage/ratio-based queries indicating a strong reliance on relative financial comparisons. 
      <strong>Panel 2</strong> shows that a few dominant operators like <em>divide</em>, <em>add</em>, and <em>subtract</em> account for the majority of program logic and thereby suggesting limited symbolic diversity.
      <strong>Panel 3</strong>’s heatmap reveals sparse and uneven company-year coverage, which may introduce temporal and entity-specific biases into learned representations. 
      <strong>Panel 4</strong> highlights that the most common answer unit is the percentage symbol (<code>%</code>) reinforcing the dominance of growth rate or ratio questions. Finally <strong>Panel 5</strong> surfaces the top 10 companies by mention frequency with <em>ETR</em> and <em>UNP</em> standing out pointing toward entity-level imbalances that may affect agent generalization.
       <p>
      These insights are critical for designing robust <strong>agentic reasoning systems</strong> especially with those leveraging retrieval, planning, and tool-augmented modules.
    </p>
      </p>
  </div>
   
</section>

    <section class="model-summary">
  <div class="container">
    <h2>LLM, CoT and Faithful CoT for ConvFinQA</h2>

    <p>
      The ConvFinQA system uses <strong>GPT-3.5 Turbo</strong> via LangChain for answering multi-step financial questions grounded in document context and table data.  Model is prompted in two styles:
      i) a direct answering mode and ii) a symbolic reasoning mode that produces evaluable math expressions.
    </p>

    <h3>LLM Details</h3>
    <ul>
      <li><strong>Model:</strong> GPT-3.5 Turbo</li>
      <li><strong>Interface:</strong> LangChain + LangSmith Tracing</li>
      <li><strong>Temperature:</strong> 0.2 (low for factual precision)</li>
    </ul>

    <h3>Prompting Strategies</h3>
    <p>The agent is guided using system instructions and annotated answer formatting:</p>

    <h4> Prompt (LLM-only):</h4>
    <pre style="background:#f4f4f4; padding:10px; border:1px solid #ccc; overflow-x: auto;">
You are a financial analyst assistant. Use the context below to answer the question.
Directly provide the final answer in &lt;ANSWER&gt;...&lt;/ANSWER&gt;.

Context:
[pre-text]

Table:
[table-text]

[post-text]

Question: [question]
Answer:
    </pre>

    <h4> Prompt (LLM + Calculator):</h4>
    <pre style="background:#f4f4f4; padding:10px; border:1px solid #ccc; overflow-x: auto;">
You are a financial analyst assistant. Use the context below to answer the question.
If calculations are needed, enclose the math expression in &lt;MATH&gt;...&lt;/MATH&gt;
and the result in &lt;ANSWER&gt;...&lt;/ANSWER&gt;.

Context:
[pre-text]

Table:
[table-text]

[post-text]

Question: [question]
Answer:
    </pre>

    <h3>Calculator Integration</h3>
    <p>
      Two versions of a calculator were tested for evaluating the math expressions generated by the LLM inside <code>&lt;MATH&gt;...&lt;/MATH&gt;</code> tags:
    </p>

    <h4> Version 1: Python <code>eval()</code> (Baseline)</h4>
    <p>
      This approach directly evaluated expressions like <code>206588 - 181001</code> using Python's built-in <code>eval()</code>:
    </p>

    <pre style="background:#f4f4f4; padding:10px; border:1px solid #ccc;">
def evaluate_expression(expr):
    try:
        return eval(expr, {"__builtins__": {}})
    except:
        return None
    </pre>

    <p>
      While effective for simple arithmetic this method poses potential safety risks if the input is not controlled.
    </p>

    <h4> Version 2: SymPy (Safe Evaluator)</h4>
    <p>
      The improved version uses the <code>sympy</code> library for safe symbolic math evaluation. It parses the expression and evaluates it numerically:
    </p>

    <pre style="background:#f4f4f4; padding:10px; border:1px solid #ccc;">
from sympy import sympify

def evaluate_expression(expr):
    try:
        result = sympify(expr).evalf()
        return float(result)
    except:
        return None
    </pre>

    <p>
      This is safer more transparent and supports symbolic computation when needed.Making it better suited for multi-step financial reasoning.
    </p>

    <h3>Why This Setup?</h3>
    <ul>
      <li>Ensures <strong>faithful reasoning</strong> over both text and tabular inputs</li>
      <li>Separates <strong>natural language</strong> and <strong>numerical computation</strong></li>
      <li>Enables symbolic traceability and numerical post-verification</li>
      <li>Improves precision on financial ratios and arithmetic operations</li>
    </ul>
  </div>
</section>


    <div class="container">
    <h1>Experiment 1: Prompting Evaluation on ConvFinQA</h1>

    <p>
      We perform a structured evaluation of <strong>GPT-3.5 Turbo</strong> on the <strong>ConvFinQA</strong> dataset by comparing two prompting strategies: 
      (1) a direct answer mode using <code>&lt;ANSWER&gt;...&lt;/ANSWER&gt;</code> and 
      (2) a symbolic reasoning mode that embeds expressions in <code>&lt;MATH&gt;...&lt;/MATH&gt;</code> tags followed by their result.
    </p>

    <p>
      It uses <code>LangChain</code>’s <code>ChatOpenAI</code> interface with <code>LangSmith</code> tracing to log each interaction. 
      The script samples 200 QA pairs from <code>train_turn.json</code>, generates prompts including pre-text, post-text, and flattened financial tables, 
      and invokes the LLM with both prompt types. If a <code>&lt;MATH&gt;</code> expression is produced it is evaluated using Python’s <code>eval()</code> in a restricted context. 
      Predicted answers are extracted, matched against gold labels using relative (3%) and absolute (0.05) tolerances, 
      and logged alongside latency, conversation type, and trace metadata.<p> Final traces with results are exported to a txt file (<code>Experiment1</code>) for accuracy analysis across conversation styles and reasoning modes.
    </p>

    <p>
  <strong>Download full trace output:</strong><br />
  <a href="Experiment1.rtf" download>Click here to download Experiment1.txt</a>
   </p>

    </p>
  </div>
    <section class="experiment-figure">
  <div class="container">
    <h2>Figure 1: Accuracy Comparison</h2>
    <img src="Experiment1.png" alt="Accuracy Comparison Bar Chart" style="max-width:100%; height:auto; border:1px solid #ccc; border-radius:8px;" />
    <p style="font-size: 14px; color: #555;">Figure: Comparison of LLM-only vs LLM + Calculator accuracy of GPT3.5 turbo on 200 ConvFinQA questions.</p>
  </div>
</section>

<section class="prompt-strategies">
  <div class="container">
    <h2>Experiment 2: Prompting Strategies Explored</h2>
    <p>We evaluated five distinct prompting strategies on the ConvFinQA dataset:</p>

    <h3>1. ExplicitCoT (Chain-of-Thought)</h3>
    <p>
      This strategy guides the model to reason step-by-step before answering, thereby improving multi-hop question accuracy.  
      Introduced in <a href="https://arxiv.org/abs/2201.11903" target="_blank">Wei et al., 2022</a> (Chain of Thought Prompting).
    </p>
    <pre style="background:#f9f9f9; padding:12px; border:1px solid #ccc; border-radius:6px;">
You are a financial analyst assistant. Explain your reasoning step by step before answering.

Context:
[pre_text]

Table:
[table_text]

[post_text]

Question: [question]
Answer:
    </pre>

    <h3>2. ReflectionCoT</h3>
    <p>
      Encourages the model to reflect on its reasoning before committing to an answer, helping correct early-stage errors.  
      Based on <a href="https://arxiv.org/abs/2303.17651" target="_blank">Madaan et al., 2023</a> (Self-Refine).
    </p>
    <pre style="background:#f9f9f9; padding:12px; border:1px solid #ccc; border-radius:6px;">
Think through the question carefully. Reflect on each aspect of the question and explain your steps before answering.

Context:
[pre_text]

Table:
[table_text]

[post_text]

Question: [question]
Answer:
    </pre>

    <h3>3. Scratchpad</h3>
    <p>
      Allows the model to "think aloud" by writing down intermediate computations before producing the final answer.  
      Inspired by <a href="https://arxiv.org/abs/2112.00114" target="_blank">Nye et al., 2021</a> (Scratchpads for Intermediate Computation).
    </p>
    <pre style="background:#f9f9f9; padding:12px; border:1px solid #ccc; border-radius:6px;">
Use the scratchpad below to write down intermediate steps, then give the final answer.

Context:
[pre_text]

Table:
[table_text]

[post_text]

Question: [question]
Answer:
    </pre>

    <h3>4. PoT (Program-of-Thought with Math Evaluation)</h3>
    <p>
      Separates language from computation by wrapping expressions in &lt;MATH&gt; and &lt;ANSWER&gt; tags for symbolic evaluation.  
      Proposed in <a href="https://arxiv.org/abs/2211.12588" target="_blank">Chen et al., 2022</a> (Program of Thoughts Prompting).
    </p>
    <pre style="background:#f9f9f9; padding:12px; border:1px solid #ccc; border-radius:6px;">
You are a financial assistant. For all math expressions, wrap them in &lt;MATH&gt;...&lt;/MATH&gt;, 
and the final result in &lt;ANSWER&gt;...&lt;/ANSWER&gt;.

Context:
[pre_text]

Table:
[table_text]

[post_text]

Question: [question]
Answer:
    </pre>

    <h3>5. EEDP (Explicit Expression Decomposition Prompting)</h3>
    <p>
      Breaks down complex queries into sub-questions to enable modular reasoning and verification of intermediate steps.  
      Inspired by <a href="https://arxiv.org/abs/2210.02406" target="_blank">Khot et al., 2022</a> (Decomposed Prompting).
    </p>
    <pre style="background:#f9f9f9; padding:12px; border:1px solid #ccc; border-radius:6px;">
Break the question into sub-questions, answer each part, and then combine the results to answer the original question.

Context:
[pre_text]

Table:
[table_text]

[post_text]

Question: [question]
Answer:
    </pre>
  </div>

  <div class="container" style="text-align: center; padding: 20px;">
  <h2>Figure 2: Comparison of Prompting Strategies</h2>
  <p style="font-size: 14px; color: #444;">
    This figure compares the performance of various prompting strategies on ConvFinQA.
    <strong>LLM + Calculator</strong> (63%) and <strong>PoT</strong> (61.5%) perform best, demonstrating that external tools and structured reasoning significantly boost accuracy.
    Mid-tier strategies like <strong>ReflectionCoT</strong>, <strong>ExplicitCoT</strong>, and <strong>Scratchpad</strong> (54–56%) offer modest improvements but fall short of the gains achieved by tool-augmented or symbolic approaches.
  </p>

  <div style="text-align: center; margin-top: 20px;">
    <img 
      src="Experiment2.png" 
      alt="Accuracy Comparison of Prompting Strategies"
      style="width: 70%; max-width: 600px; border: 1px solid #ccc; border-radius: 8px; box-shadow: 0 2px 6px rgba(0, 0, 0, 0.1);" 
    />
  </div>
</div>

<div class="container">
  <div class="agentic-prompt-description" style="margin-top: 30px;">
    <p style="font-size: 14px; color: #444;">
      We created a specialized agentic prompt inspired by the declarative and disciplined style described in the 
      <a href="https://github.com/elder-plinius/CL4R1T4S/blob/main/ANTHROPIC/Claude_4.txt" target="_blank">
        CL4R1T4S Claude 4 prompt
      </a>. 
      Our version is tailored for <strong>ConvFinQA</strong>. It enforces behavioral boundaries using declarative intent, 
      step-by-step symbolic reasoning, and scoped data usage to prevent hallucinations. The prompt structure includes 
      XML-style wrappers like <code>&lt;MATH&gt;</code> and <code>&lt;ANSWER&gt;</code> to isolate computations and results. 
      It also integrates fallback logic and explicit constraint repetition to reinforce safe and accurate responses across 
      long-form queries.
    </p>
    <pre style="background:#f9f9f9; padding:12px; border:1px solid #ccc; border-radius:6px;">
        You are FinBot, an agentic financial QA assistant.
        - Declare limitations first. Say "Insufficient data" if inputs are incomplete.
        - Reason step-by-step using symbolic CoT.
        - Use only data visible in the context. Do not hallucinate or guess.
        - Wrap math in &lt;MATH&gt;...&lt;/MATH&gt; and the final result in &lt;ANSWER&gt;...&lt;/ANSWER&gt;.
        - Follow fallback rules instead of fabricating numbers.
        
        Context:
        [pre_text]
        
        Table:
        [table_text]
        
        [post_text]
        
        Question: [question]
        Answer:
        </pre>
  </div>
</div>
<div class="container">
<div style="max-width: 100%; padding: 0 16px; box-sizing: border-box;">
  <div style="text-align: center; margin-top: 20px;">
    <img 
      src="Experiment2b.png" 
      alt="Failure of Agentic Prompt"
      style="width: 70%; max-width: 600px; border: 1px solid #ccc; border-radius: 8px; box-shadow: 0 2px 6px rgba(0, 0, 0, 0.1);" 
    />
    <p style="margin-top: 12px; font-size: 15px; color: #444;">
      <strong>Figure 2b:</strong> The Agentic prompting strategy underperforms due to its overly conservative design having discouraging estimation, forbidding hallucination, and relying strictly on visible data  which leads to a higher rate of “no-answer” outputs and lower recall.
    </p>
  </div>
</div>
</div>




    <div class="container"> 
    <h2>Experiment3: Self-Reflective Evaluation </h2>
        <p>
       We implement a <strong>self-reflective evaluation loop</strong> using <strong>GPT-3.5 Turbo</strong> to enhance its reasoning fidelity on complex financial QA tasks. 
            The model is queried via LangChain’s <code>ChatOpenAI</code> interface using a structured prompt format that supports symbolic reasoning through <code>&lt;MATH&gt;</code> and <code>&lt;ANSWER&gt;</code> tags. After generating an initial answer it is compared against the ground truth using numerical thresholds (3% relative, 0.05 absolute). 
            If incorrect then the agent is prompted to critique its own reasoning and this reflection is incorporated into a follow up prompt to encourage correction. 
            This creates a lightweight feedback loop that enables self-improvement without human intervention. 
            Traces are logged via <strong>LangSmith</strong> for full reproducibility and final results  including predictions, retries, and correctness metrics  are exported as a CSV file: <a href="langchain_self_eval-2.csv" download>langchain_self_eval.csv</a>. 
            This approach draws inspiration from the work <a href="https://arxiv.org/pdf/2203.11171" target="_blank">"Self-consistency improves for Chain-of-Thought Reasoning" (Wang et al., 2023)</a> and enables scalable evaluation of multi-step financial reasoning with minimal supervision.
            <section class="result-comparison">
              <div class="container" style="text-align: center; padding: 20px;">
                <h2>Figure 3: How does Self reflection helps ? </h2>
                <p style="font-size: 14px; color: #444;">
                  This figure compares the performance of three reasoning strategies on ConvFinQA. self-evaluating agent achieves the highest accuracy.
                </p>
                <img src="Experiment3.png" alt="Accuracy Comparison of Strategies" style="width: 70%; max-width: 600px; border: 1px solid #ccc; border-radius: 8px; box-shadow: 0 2px 6px rgba(0,0,0,0.1);" />
              </div>
            </section>

</p>

    </div>
<div class="container"> 
  <section>
    <h2>Experiment 4: Extended Reasoning with the “Wait...” Scaffolding Technique</h2>
    <p>
      Inspired by the <em>"s1: Simple Test-Time Scaling"</em> paper 
      (<a href="https://arxiv.org/abs/2501.19393" target="_blank">arXiv:2501.19393</a>), 
      which demonstrated how prompting with a simple “Wait...” token can significantly enhance reasoning in large language models, 
      I applied a similar scaffolding method to my financial QA system built on the ConvFinQA dataset. 
      In this setup, the model was instructed to begin responses with “Wait...” and enclose final answers within 
      <code>&lt;ANSWER&gt;</code> tags to encourage structured, step-by-step reasoning. 
      This intervention resulted in a <strong>notable accuracy improvement from 48.00% to 56.50%</strong>, 
      marking an <strong>8.50% gain</strong> through self-reflective retries.
    </p>

    <p>
      However, this performance still lagged behind the <strong>67.5% accuracy</strong> achieved by a dedicated reasoning agent. 
      One likely reason for this gap is that the “Wait...” prompt enforces general deliberation but doesn’t instill 
      domain-specific financial reasoning or leverage external tools, which the agent was explicitly trained to use. 
      Additionally, discrepancies in question sets or difficulty levels between experiments may have influenced the outcome, 
      making direct comparisons imperfect. These results suggest that while lightweight test-time scaffolding improves robustness, 
      its gains are bounded by task complexity and the model's reasoning priors.
    </p>

    <p>
      The evaluation results are saved in the file: 
      <a href="langchain_self_eval_waittoken.csv" download>
        <code>langchain_self_eval_waittoken.csv</code>
      </a>.
    </p>

    <h2> Figure4: Accuracy Comparison: Self-Reflection vs Scaffolding Technique</h2>
    <p>
      The bar chart below compares the performance of various strategies, including LLM-only, LLM with calculator, self-evaluating agents, and the "Wait..." scaffolding method.
    </p>
    <img src="Experiment4.png" alt="Accuracy Comparison Graph" 
         style="max-width: 100%; height: auto; border: 1px solid #ccc; border-radius: 8px; padding: 4px;">
  </section>
</div>
<div class="container">
<section>
    <h2>Experiment 5: FMP API Integration (Failure).  Drop in Accuracy</h2>

  <p>
    In this experiment we investigated whether augmenting the LLM with real-time stock price data from the Financial Modeling Prep (FMP) API would improve accuracy on price-related financial questions. Contrary to expectations, the integration led to a drop in overall accuracy compared to both the baseline LLM-only and LLM+calculator setups. While the added context was intended to provide more up-to-date price signals, it frequently introduced conflicting information when juxtaposed with the static tabular data provided in the original question. This mismatch caused the LLM to either misinterpret which price to prioritize or generate answers that inconsistently blended real-time and historical data. Additionally, the FMP-provided values often did not align with the ground truth in the ConvFinQA dataset, leading to more numeric mismatches during evaluation. <strong>Result saved to <code>stock_price_accuracy_analysis.csv</code>.</strong>
    A recurring agent failure mode observed during evaluation was the LLM’s tendency to over-assume data availability. In questions such as <em>“What is the growth rate in the common stock price from the highest price during quarter ended December 31, 2005 to the highest price during quarter ended December 31, 2006?”</em>, the model proceeded with the computation even when only one year’s data or aggregate values were available leading to hallucinated reasoning steps. Highlights the need for an intermediate reasoning layer that explicitly verifies whether all required data points are present before computation begins. Incorporating a data sufficiency check as part of the agent's pipeline could prevent such failures and substantially improve factual reliability in future multi-modal financial QA systems.
  </p>
  <p>
      <strong>Download the results:</strong> 
      <a href="stock_price_accuracy_analysis.csv" download>
        <code>stock_price_accuracy_analysis.csv</code>
      </a>
    </p>
  <p>
    I’ve written about on multiagent failures in more detail in a separate blog post: 
    <a href="https://your-blog-link.com" target="_blank">Read the full breakdown here</a>.
  </p>

    <h2> Figure5: Accuracy Comparison: API integration </h2>
    <p>
      The bar chart below compares the performance of various strategies, including LLM-only, LLM with calculator, self-evaluating llms, FMP API integration and the "Wait..." scaffolding method.
    </p>
    <img src="Experiment5.png" alt="Accuracy Comparison Graph" 
         style="max-width: 100%; height: auto; border: 1px solid #ccc; border-radius: 8px; padding: 4px;">

</section>
</div>
<div class="container">
  <section>
    <h2>Experiment 6: Qwen-14B Evaluation : Modest Gains with Calculator Augmentation but at a Cost</h2>

    <p>
      In this experiment we evaluated the performance of the Qwen-14B model loaded via Unsloth in 4-bit precision on 50 financial QA samples from the ConvFinQA dataset. The LLM-only variant of Qwen achieved an accuracy of <strong>36.0%</strong> while the <strong>Qwen + Calculator</strong> configuration improved to <strong>48.0%</strong> this way  indicating meaningful gains through structured reasoning using <code>&lt;MATH&gt;</code> and <code>&lt;ANSWER&gt;</code> tags.
       However, unlike other models evaluated Qwen exhibited <strong>verbose and latency-prone behavior</strong>, often producing unnecessarily long outputs and taking significantly more time per question. This behavior is consistent with the characteristics noted in GRPO (Group Relative Proxy Optimization) training. As confirmed in 
      <a href="https://arxiv.org/abs/2504.11343" target="_blank">arXiv:2504.11343</a>, while GRPO enhances reasoning quality it can inadvertently encourage models to generate <em>longer, more elaborate outputs</em> due to optimization over step-by-step solution paths.
    </p>

    <p>
      Due to these runtime and verbosity concerns we limited our evaluation to <strong>50 samples</strong>. It is worth noting that a larger sample size may yield a more stable estimate and potentially higher accuracy as reasoning patterns generalize better across diverse question types. While calculator scaffolding did improve factual accuracy the tradeoff in response time and verbosity could hinder deployment in latency-sensitive applications. Further fine-tuning or constraint-based decoding strategies may be required to streamline Qwen’s reasoning for production use.
    </p>

    <p>
      <strong>Download the results:</strong> 
      <a href="qwen.rtf" download>
        <code>qwen_results</code>
      </a>
    </p>

 
    <h2>Figure 6: Accuracy Comparison  Including Qwen Variants</h2>
    <p>
      The chart below visualizes performance across all evaluated strategies, now extended with Qwen-14B (LLM-only and Calculator) results. Despite modest accuracy, Qwen's output verbosity and latency highlight important tradeoffs introduced by GRPOstyle training.
    </p>
    <img src="Experiment6.png" alt="Qwen Accuracy Comparison Graph"
         style="max-width: 60%; height: auto; border: 1px solid #ccc; border-radius: 8px; padding: 4px;">
  </section>
</div>


    
<div class="container">
  <section>
    <h2>Experiment 7: RAG Benchmarking </h2>
    <p>
  ConvFinQA RAG pipeline we have implemented already demonstrates strong performance with a simpler architecture outperforming more complex agentic setups like the one proposed in 
  <a href="https://github.com/justinas-kazanavicius/ConvFinQA/tree/main" target="_blank">this repository</a>, 
  which includes a multi-node LangGraph reasoning engine. While the original LangGraph pipeline incorporates structured intermediate steps such as reranking, query rewriting, and program extraction, our direct RAG system built with ChromaDB retrieval and GPT-3.5-based answer generation which achieves comparable or better accuracy using fewer components and reduced latency.
  Specifically, our architecture reaches a numeric answer accuracy of <strong>46.33%</strong> with calculator-enhanced prompting on a held-out ConvFinQA subset (200 questions) compared to the ~<strong>39.36%</strong> accuracy (Hard Variant) reported in the reference pipeline’s best configurations. 
  In addition, our LLM-only variant still maintains an average correctness of <strong>49.2%</strong> indicating high baseline competence even without arithmetic correction
  Suggests that with a strong retriever and careful prompt engineering : a flat RAG setup can capture most of the reasoning fidelity offered by more layered agentic workflows. In future, we plan to integrate modular nodes and tracing from the reference pipeline incrementally and this is not for accuracy alone but to facilitate interpretability, extensibility,
  and plug-in tools like program verifiers and self-reflective scoring agents.
</p>
 <div class="container" style="text-align: center; margin-top: 2rem;">
  <h2>Figure 7: ConvFinQA RAG Architecture</h2>
  <img src="RAG.png" alt="ConvFinQA RAG Architecture" style="max-width: 100%; height: auto; border: 1px solid #ccc; padding: 10px; border-radius: 8px;">
</div>

<div class="container" style="text-align: center; margin-top: 2rem;">
  <h2>Figure 8:Accuracy Comparison Using Semantic, Symbolic Prompts, and Retrieval Methods</h2>
  <img src="RAG2.png" alt="Accuracy Comparison Using Semantic, Symbolic Prompts, and Retrieval Methods" style="max-width: 100%; height: auto; border: 1px solid #ccc; padding: 10px; border-radius: 8px;">
</div>

  </section>
</div>


<section class="experiment">
  <div class="container">
    <h2>Experiment8 : Guardrail-Augmented Evaluation of Financial QA</h2>
    <p> evaluation pipeline integrates agentic reasoning and robust validation through a series of symbolic and semantic <em>guardrails</em> designed to ensure consistency, safety, and interpretability in financial QA tasks. Built on top of LangChain-augmented GPT-3.5 Turbo (temperature 0.2) with LangSmith for granular trace logging the system begins by filtering each input turn through a guardrail suite that checks for the presence of a valid question, answer, and structured reasoning steps (<code>qa.steps</code> or fallback reasoning). Additional checks are performed for percentage-based answers to enforce division steps in the logic chain and also, raw reasoning strings are screened for structural coherence (presence of symbols like <code>→</code> and function-like notation). Every validated example is then passed through two prompt paths: one using a vanilla LLM query and another leveraging calculator-style prompting with <code>&lt;MATH&gt;</code> and <code>&lt;ANSWER&gt;</code> tags. Model outputs are parsed then numerically evaluated, and scored against ground-truth answers using flexible absolute (0.05) and relative (3%) tolerances. In this specific run all 200 sampled examples passed the guardrails resulting in a 0% rejection rate which is a strong signal of dataset integrity and modeling readiness.</p>
      <br>
      <p>
      <h2></h2><strong>Figure 9:</strong> Multi-layered guardrail strategy implemented in our pipeline. Inspired by <a href="https://cdn.openai.com/business-guides-and-resources/a-practical-guide-to-building-agents.pdf" target="_blank">OpenAI's Practical Guide to Building Agents</a></h2>
    </p>
    <img src="guardrails.png" alt="Guardrails Strategy Diagram" style="max-width: 100%; border: 1px solid #ccc; border-radius: 8px; margin-top: 10px;" />
  </div>
</section>

<br>
    
<section id="future-experiments" style="margin-top: 2em;">
  <h2>Future Experiments: Compression-Aware Optimization</h2>
  <p>
    Given limited computational resources we are exploring model compression as outlined in 
    <a href="https://arxiv.org/abs/2504.04342" target="_blank">  <em>"Compression Laws for Large Language Models"</em> (Sengupta et al., 2025)
    </a>. While compression increases cross-entropy loss quadratically, the drop in downstream accuracy is often linear. We plan to combine compression with prompt tuning and retrieval strategies to mitigate performance loss and thereby aiming to make the system both accurate and cost-efficient.
  </p>
</section>


  </div>
</section>


    

    
<section class="system-details">
  <div class="container">
    <h2>System Configuration and Execution Profile</h2>

    <h3>1. Hardware Configuration</h3>
    <ul>
      <li><strong>Instance Type:</strong> Cloud-based VM (Linux, x86_64)</li>
      <li><strong>GPU:</strong> NVIDIA A100 (40GB VRAM, CUDA-enabled)</li>
      <li><strong>CPU:</strong> 24 vCPUs (Intel Xeon equivalent)</li>
      <li><strong>Memory:</strong> 85GB system RAM</li>
      <li><strong>Python:</strong> Version 3.10.x (via venv)</li>
      <li><strong>Execution:</strong> Conducted locally for data preprocessing and orchestration; model inference executed via OpenAI API</li>
    </ul>

    <h3>2. Language Model & API Configuration</h3>
    <ul>
      <li><strong>Model:</strong> gpt-3.5-turbo</li>
      <li><strong>Provider:</strong> OpenAI</li>
      <li><strong>Temperature:</strong> 0.2 (to reduce output variance)</li>
      <li><strong>Retries/Timeouts:</strong> Standard OpenAI retry policy (3 retries, exponential backoff)</li>
      <li><strong>Deployment:</strong> Sequential API calls over 200 selected QA samples with five prompting paradigms</li>
    </ul>

    <h3>3. Tokenization and Usage Metrics</h3>
    <ul>
      <li><strong>Total Input Tokens:</strong> 2.7 million+</li>
      <li><strong>Total Output Tokens:</strong> 320,000+</li>
      <li><strong>Mean Tokens per Call:</strong> Input ~1800, Output ~250</li>
      <li><strong>Cost Estimation:</strong> Calculated per prompt (see below); full trace available via LangSmith logs</li>
      <li><strong>Monitoring:</strong> LangChain + LangSmith dashboards for trace tracking, token analysis, and latency logs</li>
    </ul>

    <h3>4. Latency Metrics (LangSmith)</h3>
    <ul>
      <li><strong>Total LLM Calls:</strong> 3,200+</li>
      <li><strong>P50 Latency:</strong> 1.8–2.0 seconds</li>
      <li><strong>P99 Latency:</strong> ~5.5 seconds</li>
      <li><strong>Batch Mode:</strong> Disabled – all queries evaluated independently for faithful attribution</li>
      <li><strong>Errors:</strong> None recorded across test runs</li>
    </ul>

    <h3>5. Prompting Strategies Evaluated</h3>
    <ul>
      <li><strong>ExplicitCoT:</strong> Step-by-step structured reasoning</li>
      <li><strong>ReflectionCoT:</strong> Self-reflective logic evaluation</li>
      <li><strong>Scratchpad:</strong> Intermediate variable thinking trace</li>
      <li><strong>PoT (Program-of-Thought):</strong> Symbolic + numeric calculation via XML markers</li>
      <li><strong>EEDP:</strong> Explicit Decomposition into Sub-Problems</li>
    </ul>

    <h3>6. QA Sample Evaluation</h3>
    <ul>
      <li><strong>Dataset:</strong> ConvFinQA (train_turn.json)</li>
      <li><strong>Sample Size:</strong> 200 QA pairs (filtered for full question-answer availability)</li>
      <li><strong>Accuracy Metric:</strong> Numeric agreement within tolerance (±3% relative, ±0.05 absolute)</li>
      <li><strong>Per Strategy Logging:</strong> Accuracy, predicted output, gold target, intermediate math, latency</li>
    </ul>
  </div>
</section>

<section class="cost-latency">
  <div class="container">
    <h2>Prompting Strategy Efficiency</h2>
    <p>This section compares the token cost and response latency for different prompting paradigms applied over the ConvFinQA dataset.</p>

    <table border="1" cellpadding="10" cellspacing="0">
      <thead>
        <tr>
          <th>Prompting Strategy</th>
          <th>LLM Input (truncated)</th>
          <th>Cost (USD)</th>
          <th>Latency (seconds)</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td>ExplicitCoT</td>
          <td>Break the question down into steps...</td>
          <td>$0.0009</td>
          <td>0.99</td>
        </tr>
        <tr>
          <td>ExplicitCoT</td>
          <td>Break the question down into steps...</td>
          <td>$0.00077</td>
          <td>2.37</td>
        </tr>
        <tr>
          <td>ReflectionCoT</td>
          <td>Think through the problem carefully...</td>
          <td>$0.00192</td>
          <td>1.71</td>
        </tr>
        <tr>
          <td>ReflectionCoT</td>
          <td>Think through the problem carefully...</td>
          <td>$0.00087</td>
          <td>2.88</td>
        </tr>
        <tr>
          <td>Scratchpad</td>
          <td>Use the scratchpad below to jot down...</td>
          <td>$0.00112</td>
          <td>1.85</td>
        </tr>
        <tr>
          <td>Scratchpad</td>
          <td>Use the scratchpad below to jot down...</td>
          <td>$0.00073</td>
          <td>2.48</td>
        </tr>
        <tr>
          <td>PoT</td>
          <td>&lt;MATH&gt; text{...} &lt;/MATH&gt;</td>
          <td>$0.0063</td>
          <td>9.03</td>
        </tr>
        <tr>
          <td>EEDP</td>
          <td>You are a financial assistant...</td>
          <td>$0.00118</td>
          <td>1.64</td>
        </tr>
        <tr>
          <td>EEDP</td>
          <td>You are a financial assistant...</td>
          <td>$0.00107</td>
          <td>2.31</td>
        </tr>
      </tbody>
    </table>

    <p style="margin-top: 20px;">
      <strong>Observation:</strong> The Program-of-Thought (PoT) strategy incurs higher token usage and latency due to XML formatting and symbolic evaluations. Scratchpad and EEDP show efficient trade-offs between cost and reasoning completeness.
    </p>
    <p><em>Note: Some strategies were tested under multiple configurations or trials to assess consistency in cost and latency.</em></p>
  </div>
</section>

    
    
    <section class="demo">
        <div class="container">
            <h2>Try ConvFinBot</h2>
            <p>
                Ask your financial question and the bot will give you insights based on real financial documents.
            </p>
            <button onclick="openChat()">Start Chat</button>
        </div>
    </section>

    <footer>
        <div class="container">
            <p>&copy; 2025 ConvFinBot. All rights reserved.</p>
            <p>
                View the code on
                <a href="https://github.com/sprasadhpy/ConvFinInsight" target="_blank">GitHub</a>
            </p>
        </div>
    </footer>

    <script>
        function openChat() {
            alert("The chatbot feature is under development.");
        }
    </script>
</body>

</html>
<!-- trigger redeploy -->
<!-- trigger deploy -->
